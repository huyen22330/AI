# 匯入必要的套件
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# 載入預訓練模型和分詞器
# 這裡使用 BERT 模型進行文本分類
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# 資料處理與標註
# 假設我們有一個文本資料集，我們需要將其轉換為模型可以接受的格式
train_texts = ["I love machine learning", "Transformers are great"]
train_labels = [1, 0]
train_encodings = tokenizer(train_texts, truncation=True, padding=True)

# 使用 Trainer API 進行訓練
training_args = TrainingArguments(
    output_dir='./results',          # 輸出結果存放目錄
    num_train_epochs=3,              # 訓練週期
    per_device_train_batch_size=8,   # 訓練批次大小
)

trainer = Trainer(
    model=model,                         # 需要訓練的模型
    args=training_args,                  # 訓練設置
    train_dataset=train_encodings,       # 訓練資料
)

trainer.train()  # 開始訓練



引入的套件：說明為何需要引入這些套件以及每個套件的作用。
模型加載與設定：對於 Transformers 模型的加載、設置與微調等部分，簡要說明每個步驟的目的。
處理資料：解釋資料預處理與轉換的過程，這對於處理自然語言資料尤為重要。
模型訓練：詳細說明模型訓練過程、超參數設置和如何進行優化。
推理部分：對於模型推理的部分，請解釋如何使用訓練好的模型進行預測。
