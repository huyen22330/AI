# 匯入必要的套件
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from transformers import Trainer, TrainingArguments

# 載入預訓練的模型及其分詞器
# 我們將使用一個已經訓練好的 BERT 模型進行 PEFT 微調，並且添加 Adapter 層
model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 構建 Adapter 並將其加到模型中
# PEFT 通常會添加 Adapter 層來實現參數高效的微調
adapter_config = PeftModel.get_config("adapter_config.yaml")
adapter_model = PeftModel(model, adapter_config)

# 資料處理與編碼
# 這裡我們假設我們有一個文本數據集，需要將文本轉換為模型可以接受的格式
train_texts = ["This is a great product", "This is a terrible product"]
train_labels = [1, 0]  # 標籤為1表示正面情緒，0表示負面情緒
train_encodings = tokenizer(train_texts, truncation=True, padding=True)

# 設置訓練參數
training_args = TrainingArguments(
    output_dir='./results',              # 儲存結果的目錄
    num_train_epochs=3,                  # 訓練的週期數
    per_device_train_batch_size=8,       # 每個設備的訓練批次大小
    per_device_eval_batch_size=16,       # 每個設備的評估批次大小
    logging_dir='./logs',                # 訓練過程的日誌目錄
)

# 使用 Trainer API 來微調模型
trainer = Trainer(
    model=adapter_model,                 # 使用添加了 Adapter 的模型
    args=training_args,                  # 訓練參數
    train_dataset=train_encodings,       # 訓練資料集
)

trainer.train()  # 開始訓練





引入的套件：簡單介紹為何選擇這些函式庫，並且簡述它們的功能。
PEFT 和 Adapter 模型微調的背景：簡要解釋什麼是 PEFT 和 Adapter，它們的優勢和用途。
模型加載與 Adapter 配置：如何配置 PEFT 和 Adapter 以有效微調預訓練的 Transformers 模型。
數據處理和微調過程：描述如何處理數據並進行微調，包括設置超參數、訓練過程等。
評估和推理：如何評估微調後的模型表現以及如何進行推理。
